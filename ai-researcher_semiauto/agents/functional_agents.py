from .base_agent import BaseAgent

class ResultValidationAgent(BaseAgent):
    def validate(self, response, workflow_steps=None, paper_goal=None, context=None):
        steps = workflow_steps or []
        steps_str = f"The full workflow for this research paper consists of the following steps: {steps}.\n" if steps else ""
        goal_str = f"The overall goal and requirements of the paper are: {paper_goal}.\n" if paper_goal else ""
        prompt = (
            f"You are an expert research paper quality controller.\n"
            f"{steps_str}"
            f"{goal_str}"
            "Given the following output (a section or result generated by an LLM API), judge whether it is appropriate, accurate, and fully meets the requirements and intent of the entire research paper and workflow. "
            "If the output is appropriate, output only: ok. "
            "If the output is not appropriate, first output ONLY: RESTART_TO: <StepName> (where <StepName> is the most appropriate previous step to restart from, chosen from the workflow steps above), then on a new line output a short, actionable proactive advice (in English, 1-2 sentences) to help improve the next attempt at that step. "
            "Do not use a conversational style.\n"
            f"Output to evaluate: {response}\n"
            "Your response should be either 'ok', or:\nRESTART_TO: <StepName>\nProactive advice: <your advice here>"
        )
        return self.call_llm(prompt, system_prompt="You are a research paper quality controller.") 