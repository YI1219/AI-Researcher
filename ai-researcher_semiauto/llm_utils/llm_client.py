#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LLM API å®¢æˆ·ç«¯
æä¾›ç®€å•æ˜“ç”¨çš„æ¥å£æ¥è°ƒç”¨å¤šç§å¤§è¯­è¨€æ¨¡å‹
"""

import requests
import json
import time
from typing import Dict, List, Optional, Generator
from dataclasses import dataclass
from enum import Enum
import os

class FunctionSpec:
    """å‡½æ•°è§„èŒƒ"""
    name: str
    description: str
    parameters: Dict
    
    def as_openai_tool_dict(self):
        return {
            "type": "function",
            "function": {
                "name": self.name,
                "description": self.description,
                "parameters": self.parameters
            }
        }
    
    def openai_tool_choice_dict(self):
        return {
            "type": "function",
            "function": {
                "name": self.name
            }
        }

class ModelType(Enum):
    """æ¨¡å‹ç±»å‹æšä¸¾"""
    GPT_4O = "gpt-4o"
    GPT_4O_MINI = "gpt-4o-mini"
    GPT_4_TURBO = "gpt-4-turbo"
    GPT_3_5_TURBO = "gpt-3.5-turbo"
    CLAUDE_3_5_SONNET = "claude-3-5-sonnet-20241022"
    CLAUDE_3_5_HAIKU = "claude-3-5-haiku-20241022"
    DEEPSEEK_R1 = "deepseek-r1"
    DEEPSEEK_V3 = "deepseek-v3"
    GEMINI_2_0_FLASH = "gemini-2.0-flash"
    GEMINI_2_5_PRO = "gemini-2.5-pro"
    QWEN_MAX = "qwen-max-latest"
    O1_PREVIEW = "o1-preview"
    O1_MINI = "o1-mini"

@dataclass
class ChatMessage:
    """èŠå¤©æ¶ˆæ¯"""
    role: str  # "user", "assistant", "system"
    content: str

@dataclass
class ChatResponse:
    """èŠå¤©å“åº”"""
    content: str
    model: str
    usage: Dict
    finish_reason: str
    raw_response: Dict

class LLMClient:
    """LLM API å®¢æˆ·ç«¯"""
    
    def __init__(self, api_key: str = None, base_url: str = None):
        self.api_key = api_key or "sk-8bTBbIPyXCqmvgyU83887a149eB145709c9dA58455E0F7F2"
        self.base_url = base_url or "https://api2.road2all.com/v1"
        self.headers = {
            'Authorization': f'Bearer {self.api_key}',
            'Content-Type': 'application/json'
        }
        self.session = requests.Session()
        self.session.headers.update(self.headers)
    
    def get_available_models(self) -> List[Dict]:
        """è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
        try:
            response = self.session.get(f"{self.base_url}/models", timeout=10)
            if response.status_code == 200:
                return response.json().get('data', [])
            else:
                raise Exception(f"è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {response.status_code}")
        except Exception as e:
            raise Exception(f"è·å–æ¨¡å‹åˆ—è¡¨å¼‚å¸¸: {e}")
    
    def chat(self, 
             message: str, 
             model: ModelType = ModelType.GPT_4O_MINI,
             system_prompt: str = None,
             history: List[ChatMessage] = None,
             tools: List[FunctionSpec] = None,
             tool_choice: str = None,
             temperature: float = 0.7,
             max_tokens: int = 1000,
             stream: bool = False) -> ChatResponse:
        """
        å‘é€èŠå¤©æ¶ˆæ¯
        
        Args:
            message: ç”¨æˆ·æ¶ˆæ¯
            model: ä½¿ç”¨çš„æ¨¡å‹
            system_prompt: ç³»ç»Ÿæç¤º
            history: å†å²å¯¹è¯
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
            stream: æ˜¯å¦æµå¼è¾“å‡º
        
        Returns:
            ChatResponse: èŠå¤©å“åº”
        """
        messages = []
        
        # æ·»åŠ ç³»ç»Ÿæç¤º
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        
        # æ·»åŠ å†å²å¯¹è¯
        if history:
            for msg in history:
                messages.append({"role": msg.role, "content": msg.content})
        
        # æ·»åŠ å½“å‰æ¶ˆæ¯
        messages.append({"role": "user", "content": message})
        
        payload = {
            "model": model.value,
            "messages": messages,
            "tools": tools,
            "tool_choice": tool_choice,
            "temperature": temperature,
            "max_tokens": max_tokens,
            "stream": stream
        }
        
        try:
            response = self.session.post(
                f"{self.base_url}/chat/completions",
                json=payload,
                timeout=60
            )
            
            if response.status_code == 200:
                result = response.json()
                
                if 'choices' in result and len(result['choices']) > 0:
                    choice = result['choices'][0]
                    return ChatResponse(
                        content=choice['message']['content'],
                        model=result.get('model', model.value),
                        usage=result.get('usage', {}),
                        finish_reason=choice.get('finish_reason', 'unknown'),
                        raw_response=result
                    )
                else:
                    raise Exception("å“åº”æ ¼å¼å¼‚å¸¸")
            else:
                raise Exception(f"APIè°ƒç”¨å¤±è´¥: {response.status_code}, {response.text}")
                
        except Exception as e:
            raise Exception(f"èŠå¤©è¯·æ±‚å¼‚å¸¸: {e}")
    
    def chat_stream(self, 
                   message: str, 
                   model: ModelType = ModelType.GPT_4O_MINI,
                   system_prompt: str = None,
                   history: List[ChatMessage] = None,
                   tools: List[FunctionSpec] = None,
                   tool_choice: str = None,
                   temperature: float = 0.7,
                   max_tokens: int = 1000) -> Generator[str, None, None]:
        """
        æµå¼èŠå¤©
        
        Yields:
            str: æµå¼è¾“å‡ºçš„æ–‡æœ¬ç‰‡æ®µ
        """
        messages = []
        
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        
        if history:
            for msg in history:
                messages.append({"role": msg.role, "content": msg.content})
        
        messages.append({"role": "user", "content": message})
        
        payload = {
            "model": model.value,
            "messages": messages,
            "tools": tools,
            "tool_choice": tool_choice,
            "temperature": temperature,
            "max_tokens": max_tokens,
            "stream": True
        }
        
        try:
            response = self.session.post(
                f"{self.base_url}/chat/completions",
                json=payload,
                timeout=60,
                stream=True
            )
            
            if response.status_code == 200:
                for line in response.iter_lines():
                    if line:
                        line = line.decode('utf-8')
                        if line.startswith('data: '):
                            data = line[6:]
                            if data.strip() == '[DONE]':
                                break
                            try:
                                chunk = json.loads(data)
                                if 'choices' in chunk and len(chunk['choices']) > 0:
                                    delta = chunk['choices'][0].get('delta', {})
                                    if 'content' in delta:
                                        yield delta['content']
                            except json.JSONDecodeError:
                                continue
            else:
                raise Exception(f"æµå¼è¯·æ±‚å¤±è´¥: {response.status_code}")
                
        except Exception as e:
            raise Exception(f"æµå¼èŠå¤©å¼‚å¸¸: {e}")
    
    def compare_models(self, 
                      message: str, 
                      models: List[ModelType] = None,
                      system_prompt: str = None) -> Dict[str, ChatResponse]:
        """
        æ¯”è¾ƒå¤šä¸ªæ¨¡å‹çš„å›ç­”
        
        Args:
            message: ç”¨æˆ·æ¶ˆæ¯
            models: è¦æ¯”è¾ƒçš„æ¨¡å‹åˆ—è¡¨
            system_prompt: ç³»ç»Ÿæç¤º
        
        Returns:
            Dict[str, ChatResponse]: å„æ¨¡å‹çš„å“åº”
        """
        if models is None:
            models = [
                ModelType.GPT_4O_MINI,
                ModelType.CLAUDE_3_5_HAIKU,
                ModelType.DEEPSEEK_R1,
                ModelType.GEMINI_2_0_FLASH
            ]
        
        results = {}
        
        for model in models:
            try:
                print(f"æ­£åœ¨æµ‹è¯•æ¨¡å‹: {model.value}")
                response = self.chat(
                    message=message,
                    model=model,
                    system_prompt=system_prompt,
                    max_tokens=500
                )
                results[model.value] = response
                time.sleep(0.5)  # é¿å…è¯·æ±‚è¿‡å¿«
            except Exception as e:
                print(f"æ¨¡å‹ {model.value} è°ƒç”¨å¤±è´¥: {e}")
                results[model.value] = None
        
        return results

# ä½¿ç”¨ç¤ºä¾‹
def main():
    """ä½¿ç”¨ç¤ºä¾‹"""
    client = LLMClient()
    
    print("ğŸš€ LLMå®¢æˆ·ç«¯æµ‹è¯•")
    
    # 1. ç®€å•èŠå¤©
    print("\n1. ç®€å•èŠå¤©æµ‹è¯•")
    try:
        response = client.chat("è¯·ç”¨ä¸€å¥è¯ä»‹ç»é‡å­è®¡ç®—", ModelType.GPT_4O_MINI)
        print(f"å›ç­”: {response.content}")
        print(f"Tokenä½¿ç”¨: {response.usage}")
    except Exception as e:
        print(f"é”™è¯¯: {e}")
    
    # 2. å¸¦ç³»ç»Ÿæç¤ºçš„èŠå¤©
    print("\n2. å¸¦ç³»ç»Ÿæç¤ºçš„èŠå¤©")
    try:
        response = client.chat(
            message="ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ",
            model=ModelType.CLAUDE_3_5_HAIKU,
            system_prompt="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIç ”ç©¶å‘˜ï¼Œè¯·ç”¨ç®€æ´ä¸“ä¸šçš„è¯­è¨€å›ç­”é—®é¢˜ã€‚"
        )
        print(f"å›ç­”: {response.content}")
    except Exception as e:
        print(f"é”™è¯¯: {e}")
    
    # 3. æ¨¡å‹æ¯”è¾ƒ
    print("\n3. æ¨¡å‹æ¯”è¾ƒæµ‹è¯•")
    try:
        results = client.compare_models(
            message="ç”¨ä¸€å¥è¯è§£é‡Šä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½",
            models=[ModelType.GPT_4O_MINI, ModelType.CLAUDE_3_5_HAIKU, ModelType.DEEPSEEK_R1]
        )
        
        for model_name, response in results.items():
            if response:
                print(f"\n{model_name}:")
                print(f"  å›ç­”: {response.content}")
                print(f"  Token: {response.usage.get('total_tokens', 'N/A')}")
            else:
                print(f"\n{model_name}: è°ƒç”¨å¤±è´¥")
    except Exception as e:
        print(f"é”™è¯¯: {e}")

if __name__ == "__main__":
    main() 